{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYUTuucG6g8e",
        "outputId": "9d078b94-a443-4d6f-9d29-0df9cbcd135e"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 576\u001b[0m\n\u001b[1;32m    573\u001b[0m     train_and_predict(testset, queries, epochs, batch, hid, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Example call\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 573\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(testset, queries, epochs, batch, hid)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(testset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynthetic\u001b[39m\u001b[38;5;124m'\u001b[39m, queries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, hid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m--> 573\u001b[0m     \u001b[43mtrain_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[1], line 528\u001b[0m, in \u001b[0;36mtrain_and_predict\u001b[0;34m(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_predict\u001b[39m(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# Load training and validation data\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     num_materialized_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 528\u001b[0m     dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_train_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_materialized_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     sample_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dicts[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m num_materialized_samples\n",
            "Cell \u001b[0;32mIn[1], line 444\u001b[0m, in \u001b[0;36mget_train_datasets\u001b[0;34m(num_queries, num_materialized_samples)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_train_datasets\u001b[39m(num_queries, num_materialized_samples):\n\u001b[0;32m--> 444\u001b[0m     dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_encode_train_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_materialized_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m make_dataset(\u001b[38;5;241m*\u001b[39mtrain_data, labels\u001b[38;5;241m=\u001b[39mlabels_train, max_num_joins\u001b[38;5;241m=\u001b[39mmax_num_joins,\n\u001b[1;32m    447\u001b[0m                                  max_num_predicates\u001b[38;5;241m=\u001b[39mmax_num_predicates)\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated TensorDataset for training data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[1], line 331\u001b[0m, in \u001b[0;36mload_and_encode_train_data\u001b[0;34m(num_queries, num_materialized_samples)\u001b[0m\n\u001b[1;32m    328\u001b[0m file_name_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/data/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m file_name_column_min_max_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/data/column_min_max_vals.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 331\u001b[0m joins, predicates, tables, samples, label \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_materialized_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Get column name dict\u001b[39;00m\n\u001b[1;32m    334\u001b[0m column_names \u001b[38;5;241m=\u001b[39m get_all_column_names(predicates)\n",
            "Cell \u001b[0;32mIn[1], line 288\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_name, num_materialized_samples)\u001b[0m\n\u001b[1;32m    285\u001b[0m label \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# Load queries\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    289\u001b[0m     data_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mlist\u001b[39m(rec) \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m csv\u001b[38;5;241m.\u001b[39mreader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_raw:\n",
            "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/train.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "def get_all_column_names(predicates):\n",
        "    column_names = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                column_names.add(column_name)\n",
        "    return column_names\n",
        "\n",
        "\n",
        "def get_all_table_names(tables):\n",
        "    table_names = set()\n",
        "    for query in tables:\n",
        "        for table in query:\n",
        "            table_names.add(table)\n",
        "    return table_names\n",
        "\n",
        "\n",
        "def get_all_operators(predicates):\n",
        "    operators = set()\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                operator = predicate[1]\n",
        "                operators.add(operator)\n",
        "    return operators\n",
        "\n",
        "\n",
        "def get_all_joins(joins):\n",
        "    join_set = set()\n",
        "    for query in joins:\n",
        "        for join in query:\n",
        "            join_set.add(join)\n",
        "    return join_set\n",
        "\n",
        "\n",
        "def idx_to_onehot(idx, num_elements):\n",
        "    onehot = np.zeros(num_elements, dtype=np.float32)\n",
        "    onehot[idx] = 1.\n",
        "    return onehot\n",
        "\n",
        "\n",
        "def get_set_encoding(source_set, onehot=True):\n",
        "    num_elements = len(source_set)\n",
        "    source_list = list(source_set)\n",
        "    # Sort list to avoid non-deterministic behavior\n",
        "    source_list.sort()\n",
        "    # Build map from s to i\n",
        "    thing2idx = {s: i for i, s in enumerate(source_list)}\n",
        "    # Build array (essentially a map from idx to s)\n",
        "    idx2thing = [s for i, s in enumerate(source_list)]\n",
        "    if onehot:\n",
        "        thing2vec = {s: idx_to_onehot(i, num_elements) for i, s in enumerate(source_list)}\n",
        "        return thing2vec, idx2thing\n",
        "    return thing2idx, idx2thing\n",
        "\n",
        "\n",
        "def get_min_max_vals(predicates, column_names):\n",
        "    min_max_vals = {t: [float('inf'), float('-inf')] for t in column_names}\n",
        "    for query in predicates:\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                column_name = predicate[0]\n",
        "                val = float(predicate[2])\n",
        "                if val < min_max_vals[column_name][0]:\n",
        "                    min_max_vals[column_name][0] = val\n",
        "                if val > min_max_vals[column_name][1]:\n",
        "                    min_max_vals[column_name][1] = val\n",
        "    return min_max_vals\n",
        "\n",
        "\n",
        "def normalize_data(val, column_name, column_min_max_vals):\n",
        "    min_val = column_min_max_vals[column_name][0]\n",
        "    max_val = column_min_max_vals[column_name][1]\n",
        "    val = float(val)\n",
        "    val_norm = 0.0\n",
        "    if max_val > min_val:\n",
        "        val_norm = (val - min_val) / (max_val - min_val)\n",
        "    return np.array(val_norm, dtype=np.float32)\n",
        "\n",
        "\n",
        "def normalize_labels(labels, min_val=None, max_val=None):\n",
        "    labels = np.array([np.log(float(l)) for l in labels])\n",
        "    if min_val is None:\n",
        "        min_val = labels.min()\n",
        "        print(\"min log(label): {}\".format(min_val))\n",
        "    if max_val is None:\n",
        "        max_val = labels.max()\n",
        "        print(\"max log(label): {}\".format(max_val))\n",
        "    labels_norm = (labels - min_val) / (max_val - min_val)\n",
        "    # Threshold labels\n",
        "    labels_norm = np.minimum(labels_norm, 1)\n",
        "    labels_norm = np.maximum(labels_norm, 0)\n",
        "    return labels_norm, min_val, max_val\n",
        "\n",
        "\n",
        "def unnormalize_labels(labels_norm, min_val, max_val):\n",
        "    labels_norm = np.array(labels_norm, dtype=np.float32)\n",
        "    labels = (labels_norm * (max_val - min_val)) + min_val\n",
        "    return np.array(np.round(np.exp(labels)), dtype=np.int64)\n",
        "\n",
        "\n",
        "def encode_samples(tables, samples, table2vec):\n",
        "    samples_enc = []\n",
        "    for i, query in enumerate(tables):\n",
        "        samples_enc.append(list())\n",
        "        for j, table in enumerate(query):\n",
        "            sample_vec = []\n",
        "            # Append table one-hot vector\n",
        "            sample_vec.append(table2vec[table])\n",
        "            # Append bit vector\n",
        "            sample_vec.append(samples[i][j])\n",
        "            sample_vec = np.hstack(sample_vec)\n",
        "            samples_enc[i].append(sample_vec)\n",
        "    return samples_enc\n",
        "\n",
        "\n",
        "def encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec):\n",
        "    predicates_enc = []\n",
        "    joins_enc = []\n",
        "    for i, query in enumerate(predicates):\n",
        "        predicates_enc.append(list())\n",
        "        joins_enc.append(list())\n",
        "        for predicate in query:\n",
        "            if len(predicate) == 3:\n",
        "                # Proper predicate\n",
        "                column = predicate[0]\n",
        "                operator = predicate[1]\n",
        "                val = predicate[2]\n",
        "                norm_val = normalize_data(val, column, column_min_max_vals)\n",
        "\n",
        "                pred_vec = []\n",
        "                pred_vec.append(column2vec[column])\n",
        "                pred_vec.append(op2vec[operator])\n",
        "                pred_vec.append(norm_val)\n",
        "                pred_vec = np.hstack(pred_vec)\n",
        "            else:\n",
        "                pred_vec = np.zeros((len(column2vec) + len(op2vec) + 1))\n",
        "\n",
        "            predicates_enc[i].append(pred_vec)\n",
        "\n",
        "        for predicate in joins[i]:\n",
        "            # Join instruction\n",
        "            join_vec = join2vec[predicate]\n",
        "            joins_enc[i].append(join_vec)\n",
        "    return predicates_enc, joins_enc\n",
        "\n",
        "#Modified model with CNN and residual connection\n",
        "class ModifiedSetConv(nn.Module):\n",
        "    def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "        super(ModifiedSetConv, self).__init__()\n",
        "        self.dropout_rate = 0.2  # Adjustable dropout rate\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.sample_conv1 = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.sample_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        self.predicate_conv1 = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.predicate_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        self.join_conv1 = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "        self.join_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "        # Residual connection\n",
        "        self.sample_res = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=1)\n",
        "        self.predicate_res = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=1)\n",
        "        self.join_res = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=1)\n",
        "\n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(hid_units * 3, hid_units)\n",
        "        self.fc2 = nn.Linear(hid_units, 1)\n",
        "        self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "        # Reshape input to fit Conv1d (batch_size, channels, sequence_length)\n",
        "        samples = samples.permute(0, 2, 1)\n",
        "        predicates = predicates.permute(0, 2, 1)\n",
        "        joins = joins.permute(0, 2, 1)\n",
        "\n",
        "        # Sample path\n",
        "        x_sample_res = self.sample_res(samples)\n",
        "        x_sample = F.relu(self.sample_conv1(samples))\n",
        "        x_sample = F.relu(self.sample_conv2(x_sample) + x_sample_res)  # Adding residual connection\n",
        "        x_sample = torch.sum(x_sample, dim=2) / sample_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Predicate path\n",
        "        x_predicate_res = self.predicate_res(predicates)\n",
        "        x_predicate = F.relu(self.predicate_conv1(predicates))\n",
        "        x_predicate = F.relu(self.predicate_conv2(x_predicate) + x_predicate_res)  # Adding residual connection\n",
        "        x_predicate = torch.sum(x_predicate, dim=2) / predicate_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Join path\n",
        "        x_join_res = self.join_res(joins)\n",
        "        x_join = F.relu(self.join_conv1(joins))\n",
        "        x_join = F.relu(self.join_conv2(x_join) + x_join_res)  # Adding residual connection\n",
        "        x_join = torch.sum(x_join, dim=2) / join_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "        # Combine and process final outputs\n",
        "        x = torch.cat((x_sample, x_predicate, x_join), dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        out = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "#Modified model with CNN\n",
        "# class SetConv_CNN(nn.Module):\n",
        "#     def __init__(self, sample_feats, predicate_feats, join_feats, hid_units):\n",
        "#         super(SetConv_CNN, self).__init__()\n",
        "#         self.dropout_rate = 0.2  # Adjust dropout rate as needed\n",
        "\n",
        "#         # Define convolutional layers for each path with kernel size, stride and padding appropriately selected\n",
        "#         self.sample_conv1 = nn.Conv1d(in_channels=sample_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.sample_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         self.predicate_conv1 = nn.Conv1d(in_channels=predicate_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.predicate_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         self.join_conv1 = nn.Conv1d(in_channels=join_feats, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "#         self.join_conv2 = nn.Conv1d(in_channels=hid_units, out_channels=hid_units, kernel_size=3, padding=1)\n",
        "\n",
        "#         # Define output layers\n",
        "#         self.fc1 = nn.Linear(hid_units * 3, hid_units)  # Combine features from all three paths\n",
        "#         self.fc2 = nn.Linear(hid_units, 1)\n",
        "#         self.dropout = nn.Dropout(self.dropout_rate)\n",
        "\n",
        "    # def forward(self, samples, predicates, joins, sample_mask, predicate_mask, join_mask):\n",
        "    #     # Reshape input to fit Conv1d (batch_size, channels, sequence_length)\n",
        "    #     samples = samples.permute(0, 2, 1)\n",
        "    #     predicates = predicates.permute(0, 2, 1)\n",
        "    #     joins = joins.permute(0, 2, 1)\n",
        "\n",
        "    #     # Sample path\n",
        "    #     x_sample = F.relu(self.sample_conv1(samples))\n",
        "    #     x_sample = F.relu(self.sample_conv2(x_sample))\n",
        "    #     x_sample = torch.sum(x_sample, dim=2) / sample_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Predicate path\n",
        "    #     x_predicate = F.relu(self.predicate_conv1(predicates))\n",
        "    #     x_predicate = F.relu(self.predicate_conv2(x_predicate))\n",
        "    #     x_predicate = torch.sum(x_predicate, dim=2) / predicate_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Join path\n",
        "    #     x_join = F.relu(self.join_conv1(joins))\n",
        "    #     x_join = F.relu(self.join_conv2(x_join))\n",
        "    #     x_join = torch.sum(x_join, dim=2) / join_mask.sum(1)  # Pooling and normalize\n",
        "\n",
        "    #     # Combine and process final outputs\n",
        "    #     x = torch.cat((x_sample, x_predicate, x_join), dim=1)\n",
        "    #     x = F.relu(self.fc1(x))\n",
        "    #     x = self.dropout(x)\n",
        "    #     out = torch.sigmoid(self.fc2(x))\n",
        "\n",
        "    #     return out\n",
        "\n",
        "\n",
        "def load_data(file_name, num_materialized_samples):\n",
        "    joins = []\n",
        "    predicates = []\n",
        "    tables = []\n",
        "    samples = []\n",
        "    label = []\n",
        "\n",
        "    # Load queries\n",
        "    with open(file_name + \".csv\", 'rU') as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter='#'))\n",
        "        for row in data_raw:\n",
        "            tables.append(row[0].split(','))\n",
        "            joins.append(row[1].split(','))\n",
        "            predicates.append(row[2].split(','))\n",
        "            if int(row[3]) < 1:\n",
        "                print(\"Queries must have non-zero cardinalities\")\n",
        "                exit(1)\n",
        "            label.append(row[3])\n",
        "    print(\"Loaded queries\")\n",
        "\n",
        "    # Load bitmaps\n",
        "    num_bytes_per_bitmap = int((num_materialized_samples + 7) >> 3)\n",
        "    with open(file_name + \".bitmaps\", 'rb') as f:\n",
        "        for i in range(len(tables)):\n",
        "            four_bytes = f.read(4)\n",
        "            if not four_bytes:\n",
        "                print(\"Error while reading 'four_bytes'\")\n",
        "                exit(1)\n",
        "            num_bitmaps_curr_query = int.from_bytes(four_bytes, byteorder='little')\n",
        "            bitmaps = np.empty((num_bitmaps_curr_query, num_bytes_per_bitmap * 8), dtype=np.uint8)\n",
        "            for j in range(num_bitmaps_curr_query):\n",
        "                # Read bitmap\n",
        "                bitmap_bytes = f.read(num_bytes_per_bitmap)\n",
        "                if not bitmap_bytes:\n",
        "                    print(\"Error while reading 'bitmap_bytes'\")\n",
        "                    exit(1)\n",
        "                bitmaps[j] = np.unpackbits(np.frombuffer(bitmap_bytes, dtype=np.uint8))\n",
        "            samples.append(bitmaps)\n",
        "    print(\"Loaded bitmaps\")\n",
        "\n",
        "    # Split predicates\n",
        "    predicates = [list(chunks(d, 3)) for d in predicates]\n",
        "\n",
        "    return joins, predicates, tables, samples, label\n",
        "\n",
        "\n",
        "\n",
        "def load_and_encode_train_data(num_queries, num_materialized_samples):\n",
        "    file_name_queries = \"/content/data/train\"\n",
        "    file_name_column_min_max_vals = \"/content/data/column_min_max_vals.csv\"\n",
        "\n",
        "    joins, predicates, tables, samples, label = load_data(file_name_queries, num_materialized_samples)\n",
        "\n",
        "    # Get column name dict\n",
        "    column_names = get_all_column_names(predicates)\n",
        "    column2vec, idx2column = get_set_encoding(column_names)\n",
        "\n",
        "    # Get table name dict\n",
        "    table_names = get_all_table_names(tables)\n",
        "    table2vec, idx2table = get_set_encoding(table_names)\n",
        "\n",
        "    # Get operator name dict\n",
        "    operators = get_all_operators(predicates)\n",
        "    op2vec, idx2op = get_set_encoding(operators)\n",
        "\n",
        "    # Get join name dict\n",
        "    join_set = get_all_joins(joins)\n",
        "    join2vec, idx2join = get_set_encoding(join_set)\n",
        "\n",
        "    # Get min and max values for each column\n",
        "    with open(file_name_column_min_max_vals, 'rU') as f:\n",
        "        data_raw = list(list(rec) for rec in csv.reader(f, delimiter=','))\n",
        "        column_min_max_vals = {}\n",
        "        for i, row in enumerate(data_raw):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            column_min_max_vals[row[0]] = [float(row[1]), float(row[2])]\n",
        "\n",
        "    # Get feature encoding and proper normalization\n",
        "    samples_enc = encode_samples(tables, samples, table2vec)\n",
        "    predicates_enc, joins_enc = encode_data(predicates, joins, column_min_max_vals, column2vec, op2vec, join2vec)\n",
        "    label_norm, min_val, max_val = normalize_labels(label)\n",
        "\n",
        "    # Split in training and validation samples\n",
        "    num_train = int(num_queries * 0.9)\n",
        "    num_test = num_queries - num_train\n",
        "\n",
        "    samples_train = samples_enc[:num_train]\n",
        "    predicates_train = predicates_enc[:num_train]\n",
        "    joins_train = joins_enc[:num_train]\n",
        "    labels_train = label_norm[:num_train]\n",
        "\n",
        "    samples_test = samples_enc[num_train:num_train + num_test]\n",
        "    predicates_test = predicates_enc[num_train:num_train + num_test]\n",
        "    joins_test = joins_enc[num_train:num_train + num_test]\n",
        "    labels_test = label_norm[num_train:num_train + num_test]\n",
        "\n",
        "    print(\"Number of training samples: {}\".format(len(labels_train)))\n",
        "    print(\"Number of validation samples: {}\".format(len(labels_test)))\n",
        "\n",
        "    max_num_joins = max(max([len(j) for j in joins_train]), max([len(j) for j in joins_test]))\n",
        "    max_num_predicates = max(max([len(p) for p in predicates_train]), max([len(p) for p in predicates_test]))\n",
        "\n",
        "    dicts = [table2vec, column2vec, op2vec, join2vec]\n",
        "    train_data = [samples_train, predicates_train, joins_train]\n",
        "    test_data = [samples_test, predicates_test, joins_test]\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data\n",
        "\n",
        "\n",
        "def make_dataset(samples, predicates, joins, labels, max_num_joins, max_num_predicates):\n",
        "    \"\"\"Add zero-padding and wrap as tensor dataset.\"\"\"\n",
        "\n",
        "    sample_masks = []\n",
        "    sample_tensors = []\n",
        "    for sample in samples:\n",
        "        sample_tensor = np.vstack(sample)\n",
        "        num_pad = max_num_joins + 1 - sample_tensor.shape[0]\n",
        "        sample_mask = np.ones_like(sample_tensor).mean(1, keepdims=True)\n",
        "        sample_tensor = np.pad(sample_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_mask = np.pad(sample_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        sample_tensors.append(np.expand_dims(sample_tensor, 0))\n",
        "        sample_masks.append(np.expand_dims(sample_mask, 0))\n",
        "    sample_tensors = np.vstack(sample_tensors)\n",
        "    sample_tensors = torch.FloatTensor(sample_tensors)\n",
        "    sample_masks = np.vstack(sample_masks)\n",
        "    sample_masks = torch.FloatTensor(sample_masks)\n",
        "\n",
        "    predicate_masks = []\n",
        "    predicate_tensors = []\n",
        "    for predicate in predicates:\n",
        "        predicate_tensor = np.vstack(predicate)\n",
        "        num_pad = max_num_predicates - predicate_tensor.shape[0]\n",
        "        predicate_mask = np.ones_like(predicate_tensor).mean(1, keepdims=True)\n",
        "        predicate_tensor = np.pad(predicate_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_mask = np.pad(predicate_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        predicate_tensors.append(np.expand_dims(predicate_tensor, 0))\n",
        "        predicate_masks.append(np.expand_dims(predicate_mask, 0))\n",
        "    predicate_tensors = np.vstack(predicate_tensors)\n",
        "    predicate_tensors = torch.FloatTensor(predicate_tensors)\n",
        "    predicate_masks = np.vstack(predicate_masks)\n",
        "    predicate_masks = torch.FloatTensor(predicate_masks)\n",
        "\n",
        "    join_masks = []\n",
        "    join_tensors = []\n",
        "    for join in joins:\n",
        "        join_tensor = np.vstack(join)\n",
        "        num_pad = max_num_joins - join_tensor.shape[0]\n",
        "        join_mask = np.ones_like(join_tensor).mean(1, keepdims=True)\n",
        "        join_tensor = np.pad(join_tensor, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_mask = np.pad(join_mask, ((0, num_pad), (0, 0)), 'constant')\n",
        "        join_tensors.append(np.expand_dims(join_tensor, 0))\n",
        "        join_masks.append(np.expand_dims(join_mask, 0))\n",
        "    join_tensors = np.vstack(join_tensors)\n",
        "    join_tensors = torch.FloatTensor(join_tensors)\n",
        "    join_masks = np.vstack(join_masks)\n",
        "    join_masks = torch.FloatTensor(join_masks)\n",
        "\n",
        "    target_tensor = torch.FloatTensor(labels)\n",
        "\n",
        "    return dataset.TensorDataset(sample_tensors, predicate_tensors, join_tensors, target_tensor, sample_masks,\n",
        "                                 predicate_masks, join_masks)\n",
        "\n",
        "\n",
        "def get_train_datasets(num_queries, num_materialized_samples):\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = load_and_encode_train_data(\n",
        "        num_queries, num_materialized_samples)\n",
        "    train_dataset = make_dataset(*train_data, labels=labels_train, max_num_joins=max_num_joins,\n",
        "                                 max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for training data\")\n",
        "    test_dataset = make_dataset(*test_data, labels=labels_test, max_num_joins=max_num_joins,\n",
        "                                max_num_predicates=max_num_predicates)\n",
        "    print(\"Created TensorDataset for validation data\")\n",
        "    return dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "def unnormalize_torch(vals, min_val, max_val):\n",
        "    vals = (vals * (max_val - min_val)) + min_val\n",
        "    return torch.exp(vals)\n",
        "\n",
        "\n",
        "def qerror_loss(preds, targets, min_val, max_val):\n",
        "    qerror = []\n",
        "    preds = unnormalize_torch(preds, min_val, max_val)\n",
        "    targets = unnormalize_torch(targets, min_val, max_val)\n",
        "\n",
        "    for i in range(len(targets)):\n",
        "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
        "            qerror.append(preds[i] / targets[i])\n",
        "        else:\n",
        "            qerror.append(targets[i] / preds[i])\n",
        "    return torch.mean(torch.cat(qerror))\n",
        "\n",
        "\n",
        "def predict(model, data_loader, cuda):\n",
        "    preds = []\n",
        "    t_total = 0.\n",
        "    model.eval()\n",
        "    for batch_idx, data_batch in enumerate(data_loader):\n",
        "\n",
        "        samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "        if cuda:\n",
        "            samples, predicates, joins, targets = samples.cuda(), predicates.cuda(), joins.cuda(), targets.cuda()\n",
        "            sample_masks, predicate_masks, join_masks = sample_masks.cuda(), predicate_masks.cuda(), join_masks.cuda()\n",
        "        samples, predicates, joins, targets = Variable(samples), Variable(predicates), Variable(joins), Variable(\n",
        "            targets)\n",
        "        sample_masks, predicate_masks, join_masks = Variable(sample_masks), Variable(predicate_masks), Variable(\n",
        "            join_masks)\n",
        "\n",
        "        t = time.time()\n",
        "        outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "        t_total += time.time() - t\n",
        "\n",
        "        for i in range(outputs.data.shape[0]):\n",
        "            preds.append(outputs.data[i])\n",
        "\n",
        "    return preds, t_total\n",
        "\n",
        "\n",
        "\n",
        "def print_qerror(preds_unnorm, labels_unnorm):\n",
        "    qerror = []\n",
        "    # Ensure inputs are numpy arrays for consistent handling\n",
        "    preds_unnorm = np.array(preds_unnorm, dtype=np.float32)\n",
        "    labels_unnorm = np.array(labels_unnorm, dtype=np.float32)\n",
        "\n",
        "    for i in range(len(preds_unnorm)):\n",
        "        pred = preds_unnorm[i]\n",
        "        label = labels_unnorm[i]\n",
        "        # Ensure both pred and label are scalars for division\n",
        "        if pred > label:\n",
        "            qerror.append(pred / label)\n",
        "        else:\n",
        "            qerror.append(label / pred)\n",
        "\n",
        "    qerror = np.array(qerror)  # Convert list to numpy array for statistical functions\n",
        "    print(\"Median: {}\".format(np.median(qerror)))\n",
        "    print(\"90th percentile: {}\".format(np.percentile(qerror, 90)))\n",
        "    print(\"95th percentile: {}\".format(np.percentile(qerror, 95)))\n",
        "    print(\"99th percentile: {}\".format(np.percentile(qerror, 99)))\n",
        "    print(\"Max: {}\".format(np.max(qerror)))\n",
        "    print(\"Mean: {}\".format(np.mean(qerror)))\n",
        "\n",
        "\n",
        "def train_and_predict(workload_name, num_queries, num_epochs, batch_size, hid_units, cuda=False):\n",
        "    # Load training and validation data\n",
        "    num_materialized_samples = 1000\n",
        "    dicts, column_min_max_vals, min_val, max_val, labels_train, labels_test, max_num_joins, max_num_predicates, train_data, test_data = get_train_datasets(\n",
        "        num_queries, num_materialized_samples)\n",
        "\n",
        "    # Train model\n",
        "    sample_feats = len(dicts[0]) + num_materialized_samples\n",
        "    predicate_feats = len(dicts[1]) + len(dicts[2]) + 1\n",
        "    join_feats = len(dicts[3])\n",
        "\n",
        "    model = ModifiedSetConv(sample_feats, predicate_feats, join_feats, hid_units)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Data loaders\n",
        "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # Train model\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loss_total = 0.\n",
        "\n",
        "        for batch_idx, data_batch in enumerate(train_data_loader):\n",
        "            samples, predicates, joins, targets, sample_masks, predicate_masks, join_masks = data_batch\n",
        "\n",
        "            # Run model\n",
        "            outputs = model(samples, predicates, joins, sample_masks, predicate_masks, join_masks)\n",
        "            loss = qerror_loss(outputs, targets.float(), min_val, max_val)\n",
        "            loss_total += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"Epoch {}, loss: {}\".format(epoch, loss_total / len(train_data_loader)))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds_test, _ = predict(model, test_data_loader, cuda)\n",
        "        preds_test_unnorm = unnormalize_labels(preds_test, min_val, max_val)\n",
        "        labels_test_unnorm = unnormalize_labels(labels_test, min_val, max_val)\n",
        "\n",
        "        print(\"\\nQ-Error validation set:\")\n",
        "        print_qerror(preds_test_unnorm, labels_test_unnorm)\n",
        "\n",
        "def main(testset='synthetic', queries=10000, epochs=10, batch=1024, hid=256):\n",
        "    train_and_predict(testset, queries, epochs, batch, hid, cuda=False)\n",
        "\n",
        "# Example call\n",
        "main(testset='scale', queries=6000, epochs=150, batch=400, hid=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "iuyjSNxV9xtg",
        "outputId": "1c5ea0b8-dc80-4093-f98f-06bb24f29857"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data\n",
        "epochs = list(range(150))  # Assuming 150 epochs\n",
        "loss_values = [\n",
        "    395.1378261021205, 241.07403346470423, 86.52479090009417, 51.32700783865793, 36.026547704424175,\n",
        "    29.127995491027832, 25.21650423322405, 26.037455013820104, 20.065981115613663, 13.363799231392997,\n",
        "    11.164984396525792, 10.197362491062709, 8.959369148526873, 8.111882039478846, 7.3583952358790805,\n",
        "    6.742632559367588, 6.634049892425537, 6.143478325435093, 5.807376555034092, 5.844632625579834,\n",
        "    5.591985532215664, 5.314040047781808, 5.006323678152902, 4.983042461531503, 4.797314030783517,\n",
        "    4.790436404091971, 4.66345865385873, 4.589493479047503, 4.615839770862034, 4.376040152141026,\n",
        "    4.392090371676853, 4.267454079219273, 4.071239760943821, 4.083013789994376, 4.023034266063145,\n",
        "    3.893342205456325, 3.961550303867885, 3.8238401242664883, 3.8260814802987233, 3.768895983695984,\n",
        "    3.6374368837901523, 3.667690736906869, 3.5331750086375644, 3.5213640247072493, 3.532508747918265,\n",
        "    3.529361571584429, 3.4696840729032243, 3.4282524585723877, 3.392194083758763, 3.450771995953151,\n",
        "    3.315987161227635, 3.582858715738569, 3.472845230783735, 3.341963768005371, 3.2398096663611278,\n",
        "    3.1947281701224193, 3.262454526765006, 3.201199327196394, 3.163353749683925, 3.1110603979655673,\n",
        "    3.113699878965105, 3.089980517114912, 2.9907722813742503, 3.081830246107919, 2.9563471930367604,\n",
        "    2.9997369050979614, 3.0037802968706404, 3.0078927108219693, 3.086635947227478, 2.958267160824367,\n",
        "    2.932515467916216, 2.9392327070236206, 2.8950248786381314, 2.80914763041905, 2.939070531300136,\n",
        "    2.8827366828918457, 2.906066690172468, 2.78228611605508, 2.7861547470092773, 2.798818366868155,\n",
        "    2.7751307487487793, 2.747038551739284, 2.7546065534864153, 2.7774675403322493, 2.77508544921875,\n",
        "    2.773702996117728, 2.7331090484346663, 2.8323803458895003, 2.7189941235951016, 2.7103371790477206,\n",
        "    2.7096786499023438, 2.677763206618173, 2.6101122753960744, 2.6476376397269115, 2.6617040123258318,\n",
        "    2.575154049055917, 2.6234340838023593, 2.530229023524693, 2.687024814741952, 2.581781966345651,\n",
        "    2.6392320905412947, 2.64791282585689, 2.7034865617752075, 2.574937547956194, 2.585672233785902,\n",
        "    2.6209466797964915, 2.646137765475682, 2.5753627845219205, 2.725035343851362, 2.6867771659578596,\n",
        "    2.567517246518816, 2.4998758350099837, 2.4633253812789917, 2.5374621834073747, 2.4711793831416538,\n",
        "    2.478224890572684, 2.46951230934688, 2.458404907158443, 2.4824340513774326, 2.473321693284171,\n",
        "    2.398397045476096, 2.46338905606951, 2.4625627994537354, 2.390879137175424, 2.4836734192711964,\n",
        "    2.5056286369051253, 2.5320194278444563, 2.4751067672457014, 2.4593448809215, 2.4281612804957797,\n",
        "    2.469981244632176, 2.3584043043000356, 2.4079691086496626, 2.4393217393330167, 2.462585849421365,\n",
        "    2.477842228753226, 2.5042186890329634, 2.5271664432116916, 2.619382245200021, 2.637970498629979,\n",
        "    2.5448556457247054, 2.4718018770217896, 2.4763576643807546, 2.434452874319894, 2.3880372047424316,\n",
        "    2.3996316705431258, 2.416580183165414, 2.352173089981079, 2.370722327913557, 2.3763825382505144\n",
        "]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, loss_values, marker='o', linestyle='-')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyoLQha4RfAF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
